{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Continuous Training with AutoML Vertex Pipelines with Batch Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to use Vertex AutoML pre-built components\n",
    "1. Learn how to build a Vertex AutoML pipeline with these components using BigQuery as a data source\n",
    "1. Learn how to compile, upload, and run the Vertex AutoML pipeline\n",
    "1. Serve batch predictions with BigQuery source from the AutoML pipeline\n",
    "\n",
    "\n",
    "In this lab, you will build, deploy, and run a Vertex AutoML pipeline that orchestrates the **Vertex AutoML AI** services to train, tune, and serve batch predictions to BigQuery with a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT = !(gcloud config get-value project)\n",
    "PROJECT = PROJECT[0]\n",
    "\n",
    "os.environ[\"PROJECT\"] = PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigQuery Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not gone through the KFP Walkthrough lab, you will need to run the following cell to create a BigQuery dataset and table containing the data required for this lab.\n",
    "\n",
    "**NOTE** If you already have the covertype data in a bigquery table at `<PROJECT_ID>.covertype_dataset.covertype` you may skip to **Understanding the pipeline design**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Dataset 'kylesteckler-demo:covertype_dataset'\n",
      "already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r21048939c4d82a5f_000001809057d82b_1 ... (3s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=covertype_dataset\n",
    "TABLE_ID=covertype\n",
    "DATA_SOURCE=gs://asl-public/data/covertype/dataset.csv\n",
    "SCHEMA=Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow implemented by the pipeline is defined using a Python based Domain Specific Language (DSL). The pipeline's DSL is in the `pipeline_vertex/pipeline_vertex_automl_batch_preds.py` file that we will generate below.\n",
    "\n",
    "The pipeline's DSL has been designed to avoid hardcoding any environment specific settings like file paths or connection strings. These settings are provided to the pipeline code through a set of environment variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building and deploying the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write the pipeline to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline_vertex/pipeline_vertex_automl_batch_preds.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline_vertex/pipeline_vertex_automl_batch_preds.py\n",
    "\"\"\"Kubeflow Covertype Pipeline.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from google_cloud_pipeline_components.aiplatform import (\n",
    "    AutoMLTabularTrainingJobRunOp,\n",
    "    TabularDatasetCreateOp,\n",
    "    ModelBatchPredictOp\n",
    ")\n",
    "from kfp.v2 import dsl\n",
    "\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT = os.getenv(\"PROJECT\")\n",
    "DATASET_SOURCE = os.getenv(\"DATASET_SOURCE\")\n",
    "PIPELINE_NAME = os.getenv(\"PIPELINE_NAME\", \"covertype\")\n",
    "DISPLAY_NAME = os.getenv(\"MODEL_DISPLAY_NAME\", PIPELINE_NAME)\n",
    "TARGET_COLUMN = os.getenv(\"TARGET_COLUMN\", \"Cover_Type\")\n",
    "BATCH_PREDS_SOURCE_URI = os.getenv(\"BATCH_PREDS_SOURCE_URI\")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f\"{PIPELINE_NAME}-vertex-automl-pipeline-batch-preds\",\n",
    "    description=f\"AutoML Vertex Pipeline for {PIPELINE_NAME}\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def create_pipeline():\n",
    "\n",
    "    dataset_create_task = TabularDatasetCreateOp(\n",
    "        display_name=DISPLAY_NAME,\n",
    "        bq_source=DATASET_SOURCE,\n",
    "        project=PROJECT,\n",
    "    )\n",
    "\n",
    "    automl_training_task = AutoMLTabularTrainingJobRunOp(\n",
    "        project=PROJECT,\n",
    "        display_name=DISPLAY_NAME,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        dataset=dataset_create_task.outputs[\"dataset\"],\n",
    "        target_column=TARGET_COLUMN,\n",
    "    )\n",
    "\n",
    "    batch_predict_op = ModelBatchPredictOp(\n",
    "        project=PROJECT,\n",
    "        job_display_name=\"batch_predict_job\",\n",
    "        model=automl_training_task.outputs[\"model\"],\n",
    "        bigquery_source_input_uri=BATCH_PREDS_SOURCE_URI,\n",
    "        instances_format=\"bigquery\",\n",
    "        predictions_format=\"bigquery\",\n",
    "        bigquery_destination_output_uri=f'bq://{PROJECT}',\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the ModelBatchPredictOp\n",
    "When working with an AutoML Tabular model, the ModelBatchPredictOp can take the following inputs:\n",
    "* `model`: The model resource to serve batch predictions with\n",
    "* `bigquery_source_uri`: A URI to a BigQuery table containing examples to serve batch predictions on in the format `bq://PROJECT.DATASET.TABLE`\n",
    "* `instances_format`: \"bigquery\" to serve batch predictions on BigQuery data.\n",
    "* `predictions_format`: \"bigquery\" to store the results of the batch prediction in BigQuery.\n",
    "* `bigquery_destination_output_uri`: In the format `bq://PROJECT_ID`. This is the project that the results of the batch prediction will be stored. The ModelBatchPredictOp will create a dataset in this project.\n",
    "\n",
    "Upon completion of the `ModelBatchPredictOp` you will see a new BigQuery dataset with name `prediction_<model-display-name>_<job-create-time>`. Inside this dataset you will see a `predictions` table, containing the batch prediction examples and predicted labels. If there were any errors in the batch prediction, you will also see an `errors` table. The errors table contains rows for which the prediction has failed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BigQuery table with data for batch predictions\n",
    "Before we compile and run the pipeline, let's create a BigQuery table with data we want to serve batch predictions on. To simulate \"new\" data we will simply query the existing table for all columns except the label and create a table called `newdata`. The URI to this table will be the `bigquery_source_input_uri` input to the `ModelBatchPredictOp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.01s: 100%|██████████| 3/3 [00:00<00:00, 1556.52query/s]                        \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE covertype_dataset.newdata AS \n",
    "SELECT * EXCEPT(Cover_Type)\n",
    "FROM covertype_dataset.covertype\n",
    "LIMIT 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the environment variables that will be passed to the pipeline compiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_ROOT=gs://kylesteckler-demo-kfp-artifact-store/pipeline\n",
      "env: PROJECT=kylesteckler-demo\n",
      "env: REGION=us-central1\n",
      "env: DATASET_SOURCE=bq://kylesteckler-demo.covertype_dataset.covertype\n",
      "env: BATCH_PREDS_SOURCE_URI=bq://kylesteckler-demo.covertype_dataset.newdata\n"
     ]
    }
   ],
   "source": [
    "ARTIFACT_STORE = f\"gs://{PROJECT}-kfp-artifact-store\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATASET_SOURCE = f\"bq://{PROJECT}.covertype_dataset.covertype\"\n",
    "BATCH_PREDS_SOURCE_URI = f\"bq://{PROJECT}.covertype_dataset.newdata\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT={PROJECT}\n",
    "%env REGION={REGION}\n",
    "%env DATASET_SOURCE={DATASET_SOURCE}\n",
    "%env BATCH_PREDS_SOURCE_URI={BATCH_PREDS_SOURCE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure that the `ARTIFACT_STORE` has been created, and let us create it if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://kylesteckler-demo-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the CLI compiler to compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the pipeline from the Python file we generated into a JSON description using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON = \"covertype_automl_vertex_pipeline_batch_preds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "!dsl-compile-v2 --py pipeline_vertex/pipeline_vertex_automl_batch_preds.py --output $PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can also use the Python SDK to compile the pipeline:\n",
    "\n",
    "```python\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=create_pipeline, \n",
    "    package_path=PIPELINE_JSON,\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the pipeline file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipelineSpec\": {\n",
      "    \"components\": {\n",
      "      \"comp-automltabulartrainingjob-run\": {\n",
      "        \"executorLabel\": \"exec-automltabulartrainingjob-run\",\n",
      "        \"inputDefinitions\": {\n",
      "          \"artifacts\": {\n",
      "            \"dataset\": {\n",
      "              \"artifactType\": {\n",
      "                \"schemaTitle\": \"google.VertexDataset\",\n"
     ]
    }
   ],
   "source": [
    "!head {PIPELINE_JSON}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/693210680039/locations/us-central1/pipelineJobs/covertype-vertex-automl-pipeline-batch-preds-20220504183214\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/693210680039/locations/us-central1/pipelineJobs/covertype-vertex-automl-pipeline-batch-preds-20220504183214')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/covertype-vertex-automl-pipeline-batch-preds-20220504183214?project=693210680039\n",
      "PipelineJob run completed. Resource name: projects/693210680039/locations/us-central1/pipelineJobs/covertype-vertex-automl-pipeline-batch-preds-20220504183214\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"automl_covertype_kfp_pipeline_batch_predictions\",\n",
    "    template_path=PIPELINE_JSON,\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 List of found errors:\t1.Field: batch_prediction_job.model; Message: Invalid Model resource name.\t [field_violations {\n  field: \"batch_prediction_job.model\"\n  description: \"Invalid Model resource name.\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"List of found errors:\t1.Field: batch_prediction_job.model; Message: Invalid Model resource name.\t\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:74.125.197.95:443 {grpc_message:\"List of found errors:\\t1.Field: batch_prediction_job.model; Message: Invalid Model resource name.\\t\", grpc_status:3, created_time:\"2022-11-18T14:05:02.945230531+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19811/2537317755.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mbigquery_source_input_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bq://qwiklabs-asl-01-579c20dd4e24.covertype_dataset.newdata\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mpredictions_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mbigquery_destination_output_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bq://qwiklabs-asl-01-579c20dd4e24.covertype_dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m )\n",
      "\u001b[0;32m/tmp/ipykernel_19811/2537317755.py\u001b[0m in \u001b[0;36mcreate_batch_prediction_job_bigquery_sample\u001b[0;34m(project, display_name, model_name, instances_format, bigquery_source_input_uri, predictions_format, bigquery_destination_output_uri, location, api_endpoint)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"projects/{project}/locations/{location}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     response = client.create_batch_prediction_job(\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_prediction_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_prediction_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform_v1beta1/services/job_service/client.py\u001b[0m in \u001b[0;36mcreate_batch_prediction_job\u001b[0;34m(self, request, parent, batch_prediction_job, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1870\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0;31m# Done; return the response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 List of found errors:\t1.Field: batch_prediction_job.model; Message: Invalid Model resource name.\t [field_violations {\n  field: \"batch_prediction_job.model\"\n  description: \"Invalid Model resource name.\"\n}\n]"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform_v1beta1\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "def create_batch_prediction_job_bigquery_sample(\n",
    "    project: str,\n",
    "    display_name: str,\n",
    "    model_name: str,\n",
    "    instances_format: str,\n",
    "    bigquery_source_input_uri: str,\n",
    "    predictions_format: str,\n",
    "    bigquery_destination_output_uri: str,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform_v1beta1.JobServiceClient(client_options=client_options)\n",
    "    model_parameters_dict = {}\n",
    "    model_parameters = json_format.ParseDict(model_parameters_dict, Value())\n",
    "\n",
    "    batch_prediction_job = {\n",
    "        \"display_name\": display_name,\n",
    "        # Format: 'projects/{project}/locations/{location}/models/{model_id}'\n",
    "        \"model\": model_name,\n",
    "        \"model_parameters\": model_parameters,\n",
    "        \"input_config\": {\n",
    "            \"instances_format\": instances_format,\n",
    "            \"bigquery_source\": {\"input_uri\": bigquery_source_input_uri},\n",
    "        },\n",
    "        \"output_config\": {\n",
    "            \"predictions_format\": predictions_format,\n",
    "            \"bigquery_destination\": {\n",
    "                \"output_uri\": bigquery_destination_output_uri\n",
    "            },\n",
    "        },\n",
    "        # optional\n",
    "        \"generate_explanation\": True,\n",
    "    }\n",
    "    parent = f\"projects/{project}/locations/{location}\"\n",
    "    response = client.create_batch_prediction_job(\n",
    "        parent=parent, batch_prediction_job=batch_prediction_job\n",
    "    )\n",
    "    print(\"response:\", response)\n",
    "\n",
    "\n",
    "create_batch_prediction_job_bigquery_sample(\n",
    "    project=PROJECT,\n",
    "    display_name=\"covertype_kfp_batch\",\n",
    "    model_name=\"covertype_kfp_model\",\n",
    "    instances_format=\"jsonl\",\n",
    "    bigquery_source_input_uri=\"bq://qwiklabs-asl-01-579c20dd4e24.covertype_dataset.newdata\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    bigquery_destination_output_uri=\"bq://qwiklabs-asl-01-579c20dd4e24.covertype_dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the resources created by BatchPredictOp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pipeline has finished running you will see a new BigQuery dataset with name `prediction_<model-display-name>_<job-create-time>`. Inside this dataset you will see a `predictions` table, containing the batch prediction examples and predicted labels. If there were any errors in the batch prediction, you will also see an `errors` table. The errors table contains rows for which the prediction has failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
