{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Training with Kubeflow Pipeline and Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to use KF pre-built components\n",
    "1. Learn how to use KF lightweight python components\n",
    "1. Learn how to build a KF pipeline with these components\n",
    "1. Learn how to compile, upload, and run a KF pipeline\n",
    "\n",
    "\n",
    "In this lab, you will build, deploy, and run a KFP pipeline that orchestrates the **Vertex AI** services to train, tune, and deploy a **scikit-learn** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow implemented by the pipeline is defined using a Python based Domain Specific Language (DSL). The pipeline's DSL is in the `pipeline_vertex/pipeline.py` file that we will generate below.\n",
    "\n",
    "The pipeline's DSL has been designed to avoid hardcoding any environment specific settings like file paths or connection strings. These settings are provided to the pipeline code through a set of environment variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the trainer image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training step in the pipeline will require a custom training container. The custom training image is defined in `trainer_image/Dockerfile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      "WORKDIR /app\n",
      "COPY train.py .\n",
      "\n",
      "ENTRYPOINT [\"python\", \"train.py\"]\n"
     ]
    }
   ],
   "source": [
    "!cat trainer_image_vertex/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build and push this trainer container to the container registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-asl-01-579c20dd4e24/trainer_image_covertype_vertex:latest'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_NAME = \"trainer_image_covertype_vertex\"\n",
    "TAG = \"latest\"\n",
    "TRAINING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\"\n",
    "TRAINING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 3.3 KiB before compression.\n",
      "Uploading tarball of [trainer_image_vertex] to [gs://qwiklabs-asl-01-579c20dd4e24_cloudbuild/source/1668527207.148389-60cc7e195e00490da2c03f908de4433a.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-01-579c20dd4e24/locations/global/builds/4dd8c6eb-dccf-4744-b0c1-c75d2539a7ff].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/4dd8c6eb-dccf-4744-b0c1-c75d2539a7ff?project=296524281444 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"4dd8c6eb-dccf-4744-b0c1-c75d2539a7ff\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-01-579c20dd4e24_cloudbuild/source/1668527207.148389-60cc7e195e00490da2c03f908de4433a.tgz#1668527207628458\n",
      "Copying gs://qwiklabs-asl-01-579c20dd4e24_cloudbuild/source/1668527207.148389-60cc7e195e00490da2c03f908de4433a.tgz#1668527207628458...\n",
      "/ [1 files][  1.6 KiB/  1.6 KiB]                                                \n",
      "Operation completed over 1 objects/1.6 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "eaead16dc43b: Pulling fs layer\n",
      "6529fe2a781d: Pulling fs layer\n",
      "470effa3df00: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "81f7870f3a99: Pulling fs layer\n",
      "97e572c9d0d3: Pulling fs layer\n",
      "67c86225922e: Pulling fs layer\n",
      "f0be7e3b4260: Pulling fs layer\n",
      "27519836c4fe: Pulling fs layer\n",
      "8114d85b6dca: Pulling fs layer\n",
      "4f2efc2c2115: Pulling fs layer\n",
      "8b38fa015e72: Pulling fs layer\n",
      "15b21dd7632f: Pulling fs layer\n",
      "15c1e55a1fdc: Pulling fs layer\n",
      "92394077f3fc: Pulling fs layer\n",
      "adc7c41f6a21: Pulling fs layer\n",
      "dd2f3ccdd187: Pulling fs layer\n",
      "7d108414db7f: Pulling fs layer\n",
      "c0c5eb2fe5ca: Pulling fs layer\n",
      "87f6b5e0c563: Pulling fs layer\n",
      "9993b2c47e68: Pulling fs layer\n",
      "52e0ea262be3: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "81f7870f3a99: Waiting\n",
      "97e572c9d0d3: Waiting\n",
      "67c86225922e: Waiting\n",
      "f0be7e3b4260: Waiting\n",
      "27519836c4fe: Waiting\n",
      "8114d85b6dca: Waiting\n",
      "4f2efc2c2115: Waiting\n",
      "dd2f3ccdd187: Waiting\n",
      "8b38fa015e72: Waiting\n",
      "7d108414db7f: Waiting\n",
      "c0c5eb2fe5ca: Waiting\n",
      "87f6b5e0c563: Waiting\n",
      "9993b2c47e68: Waiting\n",
      "52e0ea262be3: Waiting\n",
      "15b21dd7632f: Waiting\n",
      "15c1e55a1fdc: Waiting\n",
      "92394077f3fc: Waiting\n",
      "adc7c41f6a21: Waiting\n",
      "470effa3df00: Verifying Checksum\n",
      "470effa3df00: Download complete\n",
      "4f4fb700ef54: Download complete\n",
      "6529fe2a781d: Verifying Checksum\n",
      "6529fe2a781d: Download complete\n",
      "eaead16dc43b: Verifying Checksum\n",
      "eaead16dc43b: Download complete\n",
      "67c86225922e: Verifying Checksum\n",
      "67c86225922e: Download complete\n",
      "f0be7e3b4260: Verifying Checksum\n",
      "f0be7e3b4260: Download complete\n",
      "27519836c4fe: Verifying Checksum\n",
      "27519836c4fe: Download complete\n",
      "97e572c9d0d3: Verifying Checksum\n",
      "97e572c9d0d3: Download complete\n",
      "4f2efc2c2115: Verifying Checksum\n",
      "4f2efc2c2115: Download complete\n",
      "8b38fa015e72: Verifying Checksum\n",
      "8b38fa015e72: Download complete\n",
      "15b21dd7632f: Verifying Checksum\n",
      "15b21dd7632f: Download complete\n",
      "8114d85b6dca: Verifying Checksum\n",
      "8114d85b6dca: Download complete\n",
      "15c1e55a1fdc: Download complete\n",
      "92394077f3fc: Verifying Checksum\n",
      "92394077f3fc: Download complete\n",
      "adc7c41f6a21: Verifying Checksum\n",
      "adc7c41f6a21: Download complete\n",
      "dd2f3ccdd187: Verifying Checksum\n",
      "dd2f3ccdd187: Download complete\n",
      "7d108414db7f: Verifying Checksum\n",
      "7d108414db7f: Download complete\n",
      "c0c5eb2fe5ca: Verifying Checksum\n",
      "c0c5eb2fe5ca: Download complete\n",
      "87f6b5e0c563: Verifying Checksum\n",
      "87f6b5e0c563: Download complete\n",
      "52e0ea262be3: Verifying Checksum\n",
      "52e0ea262be3: Download complete\n",
      "81f7870f3a99: Verifying Checksum\n",
      "81f7870f3a99: Download complete\n",
      "eaead16dc43b: Pull complete\n",
      "6529fe2a781d: Pull complete\n",
      "470effa3df00: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "9993b2c47e68: Verifying Checksum\n",
      "9993b2c47e68: Download complete\n",
      "81f7870f3a99: Pull complete\n",
      "97e572c9d0d3: Pull complete\n",
      "67c86225922e: Pull complete\n",
      "f0be7e3b4260: Pull complete\n",
      "27519836c4fe: Pull complete\n",
      "8114d85b6dca: Pull complete\n",
      "4f2efc2c2115: Pull complete\n",
      "8b38fa015e72: Pull complete\n",
      "15b21dd7632f: Pull complete\n",
      "15c1e55a1fdc: Pull complete\n",
      "92394077f3fc: Pull complete\n",
      "adc7c41f6a21: Pull complete\n",
      "dd2f3ccdd187: Pull complete\n",
      "7d108414db7f: Pull complete\n",
      "c0c5eb2fe5ca: Pull complete\n",
      "87f6b5e0c563: Pull complete\n",
      "9993b2c47e68: Pull complete\n",
      "52e0ea262be3: Pull complete\n",
      "Digest: sha256:d2a00748eaa993fdb5fc941ee6695e6142b53192bff99e74b76e85a876f52379\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> f5c89f738019\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in c39f5b477a09\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 4.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 22.9 MB/s eta 0:00:00\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 32.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2022.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.1.0-py3-none-any.whl (5.8 kB)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=3e23ec53a0e11c25530fb7cd1788bb38ead77023c5f256d952b4c274e755c5e2\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/21/65/2ac62db55efa6e6edfad09f4e315aa82a35ab138f51e784fb1\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=68a9898008fa39ec7789dca182683229b657ea6ed22d5ca1652c4b0a550cb5a3\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/fb/ed/cfc98e70373dfe12db85fffab293e3153162f63de2f6aa5473\n",
      "Successfully built fire cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune, termcolor, scikit-learn, pandas, fire\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visions 0.7.5 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "statsmodels 0.13.2 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "seaborn 0.12.1 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.12.2 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.4.0 requires pandas!=1.4.0,<1.6,>1.1, but you have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-2.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container c39f5b477a09\n",
      " ---> 6e7bc8c286e3\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in a1d032cf3176\n",
      "Removing intermediate container a1d032cf3176\n",
      " ---> b28fb0f5666d\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 79d6439227c8\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 9390c6c5ba87\n",
      "Removing intermediate container 9390c6c5ba87\n",
      " ---> 76bd1de2c6c7\n",
      "Successfully built 76bd1de2c6c7\n",
      "Successfully tagged gcr.io/qwiklabs-asl-01-579c20dd4e24/trainer_image_covertype_vertex:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-asl-01-579c20dd4e24/trainer_image_covertype_vertex:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-asl-01-579c20dd4e24/trainer_image_covertype_vertex]\n",
      "fd56a01ca73a: Preparing\n",
      "7685bf0315dc: Preparing\n",
      "a14757535eb8: Preparing\n",
      "2ce63e75bec6: Preparing\n",
      "ee470c1be53d: Preparing\n",
      "affd8f958786: Preparing\n",
      "d9807b9e6598: Preparing\n",
      "159d73d7356c: Preparing\n",
      "b6a3d584445c: Preparing\n",
      "9c2c2f9a22a6: Preparing\n",
      "11e4526fac3c: Preparing\n",
      "60f7ab6b2229: Preparing\n",
      "9b55c6d23a55: Preparing\n",
      "bb078aa13c77: Preparing\n",
      "c99f4484fa81: Preparing\n",
      "14a72aa542a8: Preparing\n",
      "d207b04446b4: Preparing\n",
      "9e49809a77aa: Preparing\n",
      "9b3c3bd1f8de: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "dbf08d497500: Preparing\n",
      "83b459ce565c: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "3cab8b825015: Preparing\n",
      "b362faadeffc: Preparing\n",
      "f4462d5b2da2: Preparing\n",
      "d9807b9e6598: Waiting\n",
      "159d73d7356c: Waiting\n",
      "b6a3d584445c: Waiting\n",
      "9c2c2f9a22a6: Waiting\n",
      "11e4526fac3c: Waiting\n",
      "60f7ab6b2229: Waiting\n",
      "9b55c6d23a55: Waiting\n",
      "bb078aa13c77: Waiting\n",
      "c99f4484fa81: Waiting\n",
      "affd8f958786: Waiting\n",
      "14a72aa542a8: Waiting\n",
      "d207b04446b4: Waiting\n",
      "9e49809a77aa: Waiting\n",
      "9b3c3bd1f8de: Waiting\n",
      "dbf08d497500: Waiting\n",
      "83b459ce565c: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "b362faadeffc: Waiting\n",
      "f4462d5b2da2: Waiting\n",
      "3cab8b825015: Waiting\n",
      "ee470c1be53d: Layer already exists\n",
      "2ce63e75bec6: Layer already exists\n",
      "affd8f958786: Layer already exists\n",
      "d9807b9e6598: Layer already exists\n",
      "159d73d7356c: Layer already exists\n",
      "b6a3d584445c: Layer already exists\n",
      "9c2c2f9a22a6: Layer already exists\n",
      "11e4526fac3c: Layer already exists\n",
      "9b55c6d23a55: Layer already exists\n",
      "60f7ab6b2229: Layer already exists\n",
      "bb078aa13c77: Layer already exists\n",
      "c99f4484fa81: Layer already exists\n",
      "14a72aa542a8: Layer already exists\n",
      "d207b04446b4: Layer already exists\n",
      "9b3c3bd1f8de: Layer already exists\n",
      "9e49809a77aa: Layer already exists\n",
      "dbf08d497500: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "83b459ce565c: Layer already exists\n",
      "3cab8b825015: Layer already exists\n",
      "b362faadeffc: Layer already exists\n",
      "f4462d5b2da2: Layer already exists\n",
      "7685bf0315dc: Pushed\n",
      "fd56a01ca73a: Pushed\n",
      "a14757535eb8: Pushed\n",
      "latest: digest: sha256:9970e9d657143d095ab78b07ad441b259dc321e58149782e53e114893a28d10a size: 5754\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                        STATUS\n",
      "4dd8c6eb-dccf-4744-b0c1-c75d2539a7ff  2022-11-15T15:46:47+00:00  3M19S     gs://qwiklabs-asl-01-579c20dd4e24_cloudbuild/source/1668527207.148389-60cc7e195e00490da2c03f908de4433a.tgz  gcr.io/qwiklabs-asl-01-579c20dd4e24/trainer_image_covertype_vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINING_CONTAINER_IMAGE_URI trainer_image_vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match the ml framework version we use at training time while serving the model, we will have to supply the following serving container to the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you change the version of the training ml framework you'll have to supply a serving container with matchin version (see [pre-built containers for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building and deploying the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write the pipeline to disk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Implement\n",
    "\n",
    "1. the `train_and_deploy` function in the `pipeline_vertex/training_lightweight_component.py`\n",
    "1. the `tune_hyperparameters` function in the `pipeline_vertex/tuning_lightweight_component.py`\n",
    "\n",
    "and complete the TODOs in the `pipeline.py` file below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline_vertex/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline_vertex/pipeline.py\n",
    "# Copyright 2021 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this\n",
    "# file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "\"\"\"Kubeflow Covertype Pipeline.\"\"\"\n",
    "import os\n",
    "\n",
    "from kfp import dsl\n",
    "from training_lightweight_component import train_and_deploy\n",
    "from tuning_lightweight_component import tune_hyperparameters\n",
    "\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "\n",
    "TRAINING_CONTAINER_IMAGE_URI = os.getenv(\"TRAINING_CONTAINER_IMAGE_URI\")\n",
    "SERVING_CONTAINER_IMAGE_URI = os.getenv(\"SERVING_CONTAINER_IMAGE_URI\")\n",
    "\n",
    "TRAINING_FILE_PATH = os.getenv(\"TRAINING_FILE_PATH\")\n",
    "VALIDATION_FILE_PATH = os.getenv(\"VALIDATION_FILE_PATH\")\n",
    "\n",
    "MAX_TRIAL_COUNT = int(os.getenv(\"MAX_TRIAL_COUNT\", \"5\"))\n",
    "PARALLEL_TRIAL_COUNT = int(os.getenv(\"PARALLEL_TRIAL_COUNT\", \"5\"))\n",
    "THRESHOLD = float(os.getenv(\"THRESHOLD\", \"0.6\"))\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"covertype-kfp-pipeline\",\n",
    "    description=\"The pipeline training and deploying the Covertype classifier\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def covertype_train(\n",
    "    training_container_uri: str = TRAINING_CONTAINER_IMAGE_URI,\n",
    "    serving_container_uri: str = SERVING_CONTAINER_IMAGE_URI,\n",
    "    training_file_path: str = TRAINING_FILE_PATH,\n",
    "    validation_file_path: str = VALIDATION_FILE_PATH,\n",
    "    accuracy_deployment_threshold: float = THRESHOLD,\n",
    "    max_trial_count: int = MAX_TRIAL_COUNT,\n",
    "    parallel_trial_count: int = PARALLEL_TRIAL_COUNT,\n",
    "    pipeline_root: str = PIPELINE_ROOT,\n",
    "):\n",
    "    staging_bucket = f\"{pipeline_root}/staging\"\n",
    "\n",
    "    tuning_op = tune_hyperparameters(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        container_uri=training_container_uri,\n",
    "        training_file_path=training_file_path,\n",
    "        validation_file_path=validation_file_path,\n",
    "        staging_bucket=staging_bucket,\n",
    "        max_trial_count=max_trial_count,\n",
    "        parallel_trial_count=parallel_trial_count,\n",
    "    )\n",
    "\n",
    "    accuracy = tuning_op.outputs[\"best_accuracy\"]\n",
    "\n",
    "    with dsl.Condition(\n",
    "        accuracy >= accuracy_deployment_threshold, name=\"deploy_decision\"\n",
    "    ):\n",
    "        train_and_deploy_op =  train_and_deploy(\n",
    "            project=PROJECT_ID,\n",
    "            location=REGION,\n",
    "            container_uri=training_container_uri,\n",
    "            serving_container_uri=serving_container_uri,\n",
    "            training_file_path=training_file_path,\n",
    "            validation_file_path=validation_file_path,\n",
    "            staging_bucket=staging_bucket,\n",
    "            alpha=tuning_op.outputs[\"best_alpha\"],\n",
    "            max_iter=tuning_op.outputs[\"best_max_iter\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let stat by defining the environment variables that will be passed to the pipeline compiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_ROOT=gs://qwiklabs-asl-01-579c20dd4e24-kfp-artifact-store/pipeline\n",
      "env: PROJECT_ID=qwiklabs-asl-01-579c20dd4e24\n",
      "env: REGION=us-central1\n",
      "env: SERVING_CONTAINER_IMAGE_URI=us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-20:latest\n",
      "env: TRAINING_CONTAINER_IMAGE_URI=gcr.io/qwiklabs-asl-01-579c20dd4e24/trainer_image_covertype_vertex:latest\n",
      "env: TRAINING_FILE_PATH=gs://qwiklabs-asl-01-579c20dd4e24-kfp-artifact-store/data/training/dataset.csv\n",
      "env: VALIDATION_FILE_PATH=gs://qwiklabs-asl-01-579c20dd4e24-kfp-artifact-store/data/validation/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-kfp-artifact-store\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/training/dataset.csv\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/dataset.csv\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT_ID={PROJECT_ID}\n",
    "%env REGION={REGION}\n",
    "%env SERVING_CONTAINER_IMAGE_URI={SERVING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_CONTAINER_IMAGE_URI={TRAINING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_FILE_PATH={TRAINING_FILE_PATH}\n",
    "%env VALIDATION_FILE_PATH={VALIDATION_FILE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure that the `ARTIFACT_STORE` has been created, and let us create it if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-01-579c20dd4e24-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In case the artifact store was not created and properly set before hand, you may need\n",
    "to run in **CloudShell** the following command to allow Vertex AI to access it:\n",
    "\n",
    "```\n",
    "PROJECT_ID=$(gcloud config get-value project)\n",
    "PROJECT_NUMBER=$(gcloud projects list --filter=\"name=$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\")\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=\"serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com\" \\\n",
    "    --role=\"roles/storage.objectAdmin\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the CLI compiler to compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the pipeline from the Python file we generated into a JSON description using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON = \"covertype_kfp_pipeline.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Compile the `pipeline_vertex/pipeline.py` with the `dsl-compile-v2` command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/dsl-compile-v2\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/main.py\", line 180, in main\n",
      "    use_experimental=args.use_experimental,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/main.py\", line 164, in compile_pyfile\n",
      "    use_experimental=use_experimental,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/main.py\", line 116, in _compile_pipeline_function\n",
      "    type_check=type_check)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py\", line 1277, in compile\n",
      "    pipeline_parameters_override=pipeline_parameters)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py\", line 1196, in _create_pipeline_v2\n",
      "    pipeline_func(*args_list)\n",
      "  File \"pipeline_vertex/pipeline.py\", line 52, in covertype_train\n",
      "    notify_email_task = VertexNotificationEmailOp(recipients=[\"julian.ahrens@otto.de\"])\n",
      "NameError: name 'VertexNotificationEmailOp' is not defined\n"
     ]
    }
   ],
   "source": [
    "!dsl-compile-v2 --py pipeline_vertex/pipeline.py --output $PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can also use the Python SDK to compile the pipeline from its python function\n",
    "\n",
    "```python\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=covertype_train, \n",
    "    package_path=PIPELINE_JSON,\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the pipeline file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipelineSpec\": {\n",
      "    \"components\": {\n",
      "      \"comp-condition-deploy-decision-1\": {\n",
      "        \"dag\": {\n",
      "          \"tasks\": {\n",
      "            \"train-and-deploy\": {\n",
      "              \"cachingOptions\": {\n",
      "                \"enableCache\": true\n",
      "              },\n"
     ]
    }
   ],
   "source": [
    "!head {PIPELINE_JSON}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Upload and run the pipeline to Vertex AI using `aiplatform.PipelineJob`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/296524281444/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20221115155010\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/296524281444/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20221115155010')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/covertype-kfp-pipeline-20221115155010?project=296524281444\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20221115155010 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20221115155010 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20221115155010 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20221115155010 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20221115155010 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20221115155010 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20221115155010 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20221115155010 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/296524281444/locations/us-central1/pipelineJobs/covertype-kfp-pipeline-20221115155010\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform as vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "pipeline = vertexai.PipelineJob(\n",
    "    display_name=\"covertype_kfp_pipeline\",\n",
    "    template_path=PIPELINE_JSON,\n",
    "    enable_caching=False,\n",
    ")\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
