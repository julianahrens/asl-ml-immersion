{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFX Guided Project on Vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "\n",
    "* Learn how to generate a standard TFX template pipeline using `tfx template`\n",
    "* Learn how to modify and run a templated TFX pipeline on Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment variable setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set some environment variables to use Vertex Pipelines.\n",
    "\n",
    "Change your region if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_CLOUD_PROJECT=qwiklabs-asl-01-579c20dd4e24\n",
      "env: REGION=us-central1\n"
     ]
    }
   ],
   "source": [
    "shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "GOOGLE_CLOUD_PROJECT = shell_output[0]\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "%env GOOGLE_CLOUD_PROJECT={GOOGLE_CLOUD_PROJECT}\n",
    "%env REGION={REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Copy the predefined template to your project directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will create a working pipeline project directory and \n",
    "files by copying additional files from a predefined template.\n",
    "\n",
    "You may give your pipeline a different name by changing the PIPELINE_NAME below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"tfx-guided-project-on-vertex\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will also become the name of the project directory where your files will be put:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./tfx-guided-project-on-vertex'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_DIR = os.path.join(os.path.expanduser(\".\"), PIPELINE_NAME)\n",
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX includes the `taxi` template with the TFX python package. \n",
    "\n",
    "If you are planning to solve a point-wise prediction problem,\n",
    "including classification and regresssion, this template could be used as a starting point.\n",
    "\n",
    "The `tfx template copy` CLI command copies predefined template files into your project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Copying taxi pipeline template\n",
      "configs.py -> ./tfx-guided-project-on-vertex/pipeline/configs.py\n",
      "__init__.py -> ./tfx-guided-project-on-vertex/pipeline/__init__.py\n",
      "pipeline.py -> ./tfx-guided-project-on-vertex/pipeline/pipeline.py\n",
      "features.py -> ./tfx-guided-project-on-vertex/models/features.py\n",
      "features_test.py -> ./tfx-guided-project-on-vertex/models/features_test.py\n",
      "preprocessing_test.py -> ./tfx-guided-project-on-vertex/models/preprocessing_test.py\n",
      "model_test.py -> ./tfx-guided-project-on-vertex/models/estimator_model/model_test.py\n",
      "constants.py -> ./tfx-guided-project-on-vertex/models/estimator_model/constants.py\n",
      "__init__.py -> ./tfx-guided-project-on-vertex/models/estimator_model/__init__.py\n",
      "model.py -> ./tfx-guided-project-on-vertex/models/estimator_model/model.py\n",
      "__init__.py -> ./tfx-guided-project-on-vertex/models/__init__.py\n",
      "preprocessing.py -> ./tfx-guided-project-on-vertex/models/preprocessing.py\n",
      "model_test.py -> ./tfx-guided-project-on-vertex/models/keras_model/model_test.py\n",
      "constants.py -> ./tfx-guided-project-on-vertex/models/keras_model/constants.py\n",
      "__init__.py -> ./tfx-guided-project-on-vertex/models/keras_model/__init__.py\n",
      "model.py -> ./tfx-guided-project-on-vertex/models/keras_model/model.py\n",
      "model_analysis.ipynb -> ./tfx-guided-project-on-vertex/model_analysis.ipynb\n",
      "kubeflow_v2_runner.py -> ./tfx-guided-project-on-vertex/kubeflow_v2_runner.py\n",
      "data_validation.ipynb -> ./tfx-guided-project-on-vertex/data_validation.ipynb\n",
      "kubeflow_runner.py -> ./tfx-guided-project-on-vertex/kubeflow_runner.py\n",
      "__init__.py -> ./tfx-guided-project-on-vertex/__init__.py\n",
      ".gitignore -> ./tfx-guided-project-on-vertex/.gitignore\n",
      "local_runner.py -> ./tfx-guided-project-on-vertex/local_runner.py\n"
     ]
    }
   ],
   "source": [
    "!tfx template copy \\\n",
    "  --pipeline-name={PIPELINE_NAME} \\\n",
    "  --destination-path={PROJECT_DIR} \\\n",
    "  --model=taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need to build the Docker container that will run the TFX components on Vertex and push it to the Google Cloud Registry associated with the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-asl-01-579c20dd4e24/tfx-guided-project-on-vertex'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Docker image name for the pipeline image.\n",
    "CUSTOM_TFX_IMAGE = f\"gcr.io/{GOOGLE_CLOUD_PROJECT}/{PIPELINE_NAME}\"\n",
    "CUSTOM_TFX_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move into the TFX project scaffold generated by `tfx template` and create a `Dockerfile` in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/tfx-guided-project-on-vertex\n"
     ]
    }
   ],
   "source": [
    "%cd {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM gcr.io/tfx-oss-public/tfx:1.4.0\n",
    "\n",
    "RUN pip install -U pip\n",
    "RUN pip install google-cloud-aiplatform==1.7.1 kfp==1.8.1\n",
    "\n",
    "WORKDIR /pipeline\n",
    "COPY . ./\n",
    "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build and push the container: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 24 file(s) totalling 1.9 MiB before compression.\n",
      "Some files were not included in the source upload.\n",
      "\n",
      "Check the gcloud log [/home/jupyter/.config/gcloud/logs/2022.11.18/09.44.14.173230.log] to see which files and the contents of the\n",
      "default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn\n",
      "more).\n",
      "\n",
      "Uploading tarball of [.] to [gs://qwiklabs-asl-01-579c20dd4e24_cloudbuild/source/1668764654.268241-df2fc4a287bc47cfa095a7bb68431d61.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-01-579c20dd4e24/locations/global/builds/41d9c60f-43d9-4325-a4d8-19e1744a6de8].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/41d9c60f-43d9-4325-a4d8-19e1744a6de8?project=296524281444 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"41d9c60f-43d9-4325-a4d8-19e1744a6de8\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-01-579c20dd4e24_cloudbuild/source/1668764654.268241-df2fc4a287bc47cfa095a7bb68431d61.tgz#1668764654942579\n",
      "Copying gs://qwiklabs-asl-01-579c20dd4e24_cloudbuild/source/1668764654.268241-df2fc4a287bc47cfa095a7bb68431d61.tgz#1668764654942579...\n",
      "/ [1 files][320.7 KiB/320.7 KiB]                                                \n",
      "Operation completed over 1 objects/320.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.014MB\n",
      "Step 1/6 : FROM gcr.io/tfx-oss-public/tfx:1.4.0\n",
      "1.4.0: Pulling from tfx-oss-public/tfx\n",
      "e4ca327ec0e7: Pulling fs layer\n",
      "0fa9fc055636: Pulling fs layer\n",
      "448bb2d7fba5: Pulling fs layer\n",
      "a084e2627368: Pulling fs layer\n",
      "de932d3a14d8: Pulling fs layer\n",
      "ebe7db8e97e0: Pulling fs layer\n",
      "66fef8aabad3: Pulling fs layer\n",
      "9696f5331161: Pulling fs layer\n",
      "7799e2177407: Pulling fs layer\n",
      "56d35ebee226: Pulling fs layer\n",
      "f23d5847df7e: Pulling fs layer\n",
      "85729b3aa914: Pulling fs layer\n",
      "27594cd25c9b: Pulling fs layer\n",
      "420dbd17143e: Pulling fs layer\n",
      "71797a45a0de: Pulling fs layer\n",
      "b073ad8b8421: Pulling fs layer\n",
      "4b7516634a4b: Pulling fs layer\n",
      "434af55afd20: Pulling fs layer\n",
      "85d42009e725: Pulling fs layer\n",
      "6d895d7b587a: Pulling fs layer\n",
      "9ff76d709282: Pulling fs layer\n",
      "4160ec915caf: Pulling fs layer\n",
      "eb84f1e37919: Pulling fs layer\n",
      "a616daa88ca5: Pulling fs layer\n",
      "99b8f7435830: Pulling fs layer\n",
      "1e2f495eab09: Pulling fs layer\n",
      "d880b09bf9ec: Pulling fs layer\n",
      "403fe241a00b: Pulling fs layer\n",
      "ca533ed5e802: Pulling fs layer\n",
      "9d118b4c5fa6: Pulling fs layer\n",
      "388915f48178: Pulling fs layer\n",
      "ddeefb06ab95: Pulling fs layer\n",
      "62d0755e4559: Pulling fs layer\n",
      "58a229d727c0: Pulling fs layer\n",
      "c4606a99c9de: Pulling fs layer\n",
      "a084e2627368: Waiting\n",
      "de932d3a14d8: Waiting\n",
      "ebe7db8e97e0: Waiting\n",
      "66fef8aabad3: Waiting\n",
      "9696f5331161: Waiting\n",
      "7799e2177407: Waiting\n",
      "56d35ebee226: Waiting\n",
      "f23d5847df7e: Waiting\n",
      "85729b3aa914: Waiting\n",
      "27594cd25c9b: Waiting\n",
      "420dbd17143e: Waiting\n",
      "71797a45a0de: Waiting\n",
      "b073ad8b8421: Waiting\n",
      "4b7516634a4b: Waiting\n",
      "434af55afd20: Waiting\n",
      "85d42009e725: Waiting\n",
      "6d895d7b587a: Waiting\n",
      "9ff76d709282: Waiting\n",
      "4160ec915caf: Waiting\n",
      "eb84f1e37919: Waiting\n",
      "a616daa88ca5: Waiting\n",
      "99b8f7435830: Waiting\n",
      "1e2f495eab09: Waiting\n",
      "d880b09bf9ec: Waiting\n",
      "403fe241a00b: Waiting\n",
      "ca533ed5e802: Waiting\n",
      "9d118b4c5fa6: Waiting\n",
      "388915f48178: Waiting\n",
      "ddeefb06ab95: Waiting\n",
      "62d0755e4559: Waiting\n",
      "58a229d727c0: Waiting\n",
      "c4606a99c9de: Waiting\n",
      "0fa9fc055636: Verifying Checksum\n",
      "0fa9fc055636: Download complete\n",
      "448bb2d7fba5: Verifying Checksum\n",
      "448bb2d7fba5: Download complete\n",
      "a084e2627368: Verifying Checksum\n",
      "a084e2627368: Download complete\n",
      "de932d3a14d8: Verifying Checksum\n",
      "de932d3a14d8: Download complete\n",
      "e4ca327ec0e7: Verifying Checksum\n",
      "e4ca327ec0e7: Download complete\n",
      "66fef8aabad3: Verifying Checksum\n",
      "66fef8aabad3: Download complete\n",
      "7799e2177407: Download complete\n",
      "e4ca327ec0e7: Pull complete\n",
      "0fa9fc055636: Pull complete\n",
      "448bb2d7fba5: Pull complete\n",
      "a084e2627368: Pull complete\n",
      "de932d3a14d8: Pull complete\n",
      "56d35ebee226: Verifying Checksum\n",
      "56d35ebee226: Download complete\n",
      "f23d5847df7e: Verifying Checksum\n",
      "f23d5847df7e: Download complete\n",
      "9696f5331161: Verifying Checksum\n",
      "9696f5331161: Download complete\n",
      "ebe7db8e97e0: Verifying Checksum\n",
      "ebe7db8e97e0: Download complete\n",
      "27594cd25c9b: Verifying Checksum\n",
      "27594cd25c9b: Download complete\n",
      "71797a45a0de: Verifying Checksum\n",
      "71797a45a0de: Download complete\n",
      "420dbd17143e: Verifying Checksum\n",
      "420dbd17143e: Download complete\n",
      "4b7516634a4b: Download complete\n",
      "434af55afd20: Verifying Checksum\n",
      "434af55afd20: Download complete\n",
      "85d42009e725: Verifying Checksum\n",
      "85d42009e725: Download complete\n",
      "6d895d7b587a: Verifying Checksum\n",
      "6d895d7b587a: Download complete\n",
      "85729b3aa914: Verifying Checksum\n",
      "85729b3aa914: Download complete\n",
      "4160ec915caf: Verifying Checksum\n",
      "4160ec915caf: Download complete\n",
      "9ff76d709282: Verifying Checksum\n",
      "9ff76d709282: Download complete\n",
      "a616daa88ca5: Verifying Checksum\n",
      "a616daa88ca5: Download complete\n",
      "eb84f1e37919: Verifying Checksum\n",
      "eb84f1e37919: Download complete\n",
      "99b8f7435830: Verifying Checksum\n",
      "99b8f7435830: Download complete\n",
      "b073ad8b8421: Verifying Checksum\n",
      "b073ad8b8421: Download complete\n",
      "403fe241a00b: Verifying Checksum\n",
      "403fe241a00b: Download complete\n",
      "1e2f495eab09: Download complete\n",
      "9d118b4c5fa6: Download complete\n",
      "388915f48178: Verifying Checksum\n",
      "388915f48178: Download complete\n",
      "ddeefb06ab95: Verifying Checksum\n",
      "ddeefb06ab95: Download complete\n",
      "62d0755e4559: Verifying Checksum\n",
      "62d0755e4559: Download complete\n",
      "58a229d727c0: Verifying Checksum\n",
      "58a229d727c0: Download complete\n",
      "ca533ed5e802: Verifying Checksum\n",
      "ca533ed5e802: Download complete\n",
      "c4606a99c9de: Verifying Checksum\n",
      "c4606a99c9de: Download complete\n",
      "d880b09bf9ec: Verifying Checksum\n",
      "d880b09bf9ec: Download complete\n",
      "ebe7db8e97e0: Pull complete\n",
      "66fef8aabad3: Pull complete\n",
      "9696f5331161: Pull complete\n",
      "7799e2177407: Pull complete\n",
      "56d35ebee226: Pull complete\n",
      "f23d5847df7e: Pull complete\n",
      "85729b3aa914: Pull complete\n",
      "27594cd25c9b: Pull complete\n",
      "420dbd17143e: Pull complete\n",
      "71797a45a0de: Pull complete\n",
      "b073ad8b8421: Pull complete\n",
      "4b7516634a4b: Pull complete\n",
      "434af55afd20: Pull complete\n",
      "85d42009e725: Pull complete\n",
      "6d895d7b587a: Pull complete\n",
      "9ff76d709282: Pull complete\n",
      "4160ec915caf: Pull complete\n",
      "eb84f1e37919: Pull complete\n",
      "a616daa88ca5: Pull complete\n",
      "99b8f7435830: Pull complete\n",
      "1e2f495eab09: Pull complete\n",
      "d880b09bf9ec: Pull complete\n",
      "403fe241a00b: Pull complete\n",
      "ca533ed5e802: Pull complete\n",
      "9d118b4c5fa6: Pull complete\n",
      "388915f48178: Pull complete\n",
      "ddeefb06ab95: Pull complete\n",
      "62d0755e4559: Pull complete\n",
      "58a229d727c0: Pull complete\n",
      "c4606a99c9de: Pull complete\n",
      "Digest: sha256:1c90d7c7df1d78147013ae8d0377b1496b211391d740b88da6d9892946c30fb1\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.4.0\n",
      " ---> 8badc9bc28b6\n",
      "Step 2/6 : RUN pip install -U pip\n",
      " ---> Running in bf14aa5e0ee9\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (21.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.3.1\n",
      "    Uninstalling pip-21.3.1:\n",
      "      Successfully uninstalled pip-21.3.1\n",
      "Successfully installed pip-22.3.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container bf14aa5e0ee9\n",
      " ---> 7ee2ee8a0522\n",
      "Step 3/6 : RUN pip install google-cloud-aiplatform==1.7.1 kfp==1.8.1\n",
      " ---> Running in 0267c25fd847\n",
      "Collecting google-cloud-aiplatform==1.7.1\n",
      "  Downloading google_cloud_aiplatform-1.7.1-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 12.0 MB/s eta 0:00:00\n",
      "Collecting kfp==1.8.1\n",
      "  Downloading kfp-1.8.1.tar.gz (248 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 248.5/248.5 kB 31.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.1) (1.19.7)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.1) (2.30.1)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.1) (1.31.4)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.1) (20.9)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.7.1) (1.42.3)\n",
      "Collecting absl-py<=0.11,>=0.9\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.8/127.8 kB 16.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (5.4.1)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (12.0.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (1.12.8)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (1.35.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting cloudpickle<2,>=1.3.0\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 8.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 9.6 MB/s eta 0:00:00\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: click<8,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (7.1.2)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (0.1.13)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 14.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (3.19.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.1) (1.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<=0.11,>=0.9->kfp==1.8.1) (1.15.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.1) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.1) (1.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.1) (58.5.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.1) (1.53.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.1) (2021.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.1) (2.25.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.1) (1.41.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.1) (0.1.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.1) (3.0.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.1) (0.19.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.1) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.1) (1.7.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.1) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.1) (2.8.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.1) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.1) (0.18.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.1) (4.8.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.1) (1.26.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.1) (2021.10.8)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.1) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.1) (0.57.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.7.1) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from pydantic<2,>=1.8.2->kfp==1.8.1) (3.7.4.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.1) (0.37.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.1) (1.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.1) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform==1.7.1) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.8.1) (3.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.1) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.1) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.7.1) (2.20)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.1-py3-none-any.whl size=345340 sha256=b96ea3a289ffc2a17cd3ca81b58a114fb71856ea113f6568c8e303b5180eb75d\n",
      "  Stored in directory: /root/.cache/pip/wheels/a0/f5/b5/ec52eb75a8cbf994e7b2c8405e17f5175460e10b7c2cbb44cc\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=45a0fdf7c0114231331635e3d4272023a1fc99a70c05e42b08b5fe03ff9a70b2\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/21/65/2ac62db55efa6e6edfad09f4e315aa82a35ab138f51e784fb1\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99715 sha256=a99c4538a6e7310628c5a2caef610c1dbf8046667e72fb47f1399ad9f82e8b75\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/f0/9d/dcf844b38d247b944657d0b9e8d55b76baef9bfdeb680e4596\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=fffef9c0296e69a3c5c241fb80152b17f9225fa4104f58e87733dc647a93bf31\n",
      "  Stored in directory: /root/.cache/pip/wheels/5a/4e/89/a4493443c6aadb8a38e9709610efc3bfafaea3a4108df4c112\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: tabulate, strip-hints, fire, docstring-parser, Deprecated, cloudpickle, absl-py, requests-toolbelt, kfp-server-api, jsonschema, kfp, google-cloud-aiplatform\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.0.0\n",
      "    Uninstalling cloudpickle-2.0.0:\n",
      "      Successfully uninstalled cloudpickle-2.0.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.12.0\n",
      "    Uninstalling absl-py-0.12.0:\n",
      "      Successfully uninstalled absl-py-0.12.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.1.2\n",
      "    Uninstalling jsonschema-4.1.2:\n",
      "      Successfully uninstalled jsonschema-4.1.2\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.6.2\n",
      "    Uninstalling google-cloud-aiplatform-1.6.2:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.6.2\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.18.0 requires tensorflow-io-gcs-filesystem==0.18.0, which is not installed.\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\n",
      "tensorflow-io 0.18.0 requires tensorflow<2.6.0,>=2.5.0, but you have tensorflow 2.6.2 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 absl-py-0.11.0 cloudpickle-1.6.0 docstring-parser-0.15 fire-0.4.0 google-cloud-aiplatform-1.7.1 jsonschema-3.2.0 kfp-1.8.1 kfp-server-api-1.8.5 requests-toolbelt-0.10.1 strip-hints-0.1.10 tabulate-0.9.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 0267c25fd847\n",
      " ---> 54891853ad02\n",
      "Step 4/6 : WORKDIR /pipeline\n",
      " ---> Running in 05e7fdef1b39\n",
      "Removing intermediate container 05e7fdef1b39\n",
      " ---> 2d3cb60cf09f\n",
      "Step 5/6 : COPY . ./\n",
      " ---> 577e13bf90b9\n",
      "Step 6/6 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      " ---> Running in 0e5cef779f7c\n",
      "Removing intermediate container 0e5cef779f7c\n",
      " ---> b535ea109302\n",
      "Successfully built b535ea109302\n",
      "Successfully tagged gcr.io/qwiklabs-asl-01-579c20dd4e24/tfx-guided-project-on-vertex:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-asl-01-579c20dd4e24/tfx-guided-project-on-vertex\n",
      "The push refers to repository [gcr.io/qwiklabs-asl-01-579c20dd4e24/tfx-guided-project-on-vertex]\n",
      "a6b182db6a4e: Preparing\n",
      "da18426570b0: Preparing\n",
      "89d38117114e: Preparing\n",
      "88a82020f9c7: Preparing\n",
      "47eadc53848d: Preparing\n",
      "360afeadb83c: Preparing\n",
      "610c8c70c225: Preparing\n",
      "11aaa1ad9bbe: Preparing\n",
      "c1540b4fd089: Preparing\n",
      "5be20e773bf8: Preparing\n",
      "f0aed3b553c7: Preparing\n",
      "e55c134eccb1: Preparing\n",
      "b3e660ff020a: Preparing\n",
      "1795fd5db4de: Preparing\n",
      "dad41956ce77: Preparing\n",
      "ab61683fbd20: Preparing\n",
      "117941aa2d3f: Preparing\n",
      "3a4c764c9a45: Preparing\n",
      "34db85e87fb0: Preparing\n",
      "782857218c62: Preparing\n",
      "195f2f019955: Preparing\n",
      "9546e4c87db8: Preparing\n",
      "78613865172b: Preparing\n",
      "97676aba1403: Preparing\n",
      "0fb4d6eba0b6: Preparing\n",
      "e8b9f9e4195c: Preparing\n",
      "eb1f25078652: Preparing\n",
      "b7ce5de9980f: Preparing\n",
      "0796fbb63752: Preparing\n",
      "1233ef617acb: Preparing\n",
      "390ace10d575: Preparing\n",
      "c1b078f0e002: Preparing\n",
      "70720fcb790d: Preparing\n",
      "e05247f7d2f8: Preparing\n",
      "ab7c37becce8: Preparing\n",
      "5ccc1cf20429: Preparing\n",
      "b2d48dbbbef2: Preparing\n",
      "260b0b2ff58a: Preparing\n",
      "6babb56be259: Preparing\n",
      "360afeadb83c: Waiting\n",
      "610c8c70c225: Waiting\n",
      "11aaa1ad9bbe: Waiting\n",
      "c1540b4fd089: Waiting\n",
      "5be20e773bf8: Waiting\n",
      "f0aed3b553c7: Waiting\n",
      "e55c134eccb1: Waiting\n",
      "b3e660ff020a: Waiting\n",
      "1795fd5db4de: Waiting\n",
      "dad41956ce77: Waiting\n",
      "ab61683fbd20: Waiting\n",
      "117941aa2d3f: Waiting\n",
      "3a4c764c9a45: Waiting\n",
      "34db85e87fb0: Waiting\n",
      "782857218c62: Waiting\n",
      "195f2f019955: Waiting\n",
      "9546e4c87db8: Waiting\n",
      "78613865172b: Waiting\n",
      "97676aba1403: Waiting\n",
      "0fb4d6eba0b6: Waiting\n",
      "e8b9f9e4195c: Waiting\n",
      "eb1f25078652: Waiting\n",
      "b7ce5de9980f: Waiting\n",
      "0796fbb63752: Waiting\n",
      "1233ef617acb: Waiting\n",
      "390ace10d575: Waiting\n",
      "c1b078f0e002: Waiting\n",
      "70720fcb790d: Waiting\n",
      "e05247f7d2f8: Waiting\n",
      "ab7c37becce8: Waiting\n",
      "5ccc1cf20429: Waiting\n",
      "b2d48dbbbef2: Waiting\n",
      "260b0b2ff58a: Waiting\n",
      "6babb56be259: Waiting\n",
      "47eadc53848d: Layer already exists\n",
      "360afeadb83c: Layer already exists\n",
      "610c8c70c225: Layer already exists\n",
      "11aaa1ad9bbe: Layer already exists\n",
      "c1540b4fd089: Layer already exists\n",
      "da18426570b0: Pushed\n",
      "5be20e773bf8: Layer already exists\n",
      "a6b182db6a4e: Pushed\n",
      "f0aed3b553c7: Layer already exists\n",
      "e55c134eccb1: Layer already exists\n",
      "b3e660ff020a: Layer already exists\n",
      "1795fd5db4de: Layer already exists\n",
      "dad41956ce77: Layer already exists\n",
      "ab61683fbd20: Layer already exists\n",
      "117941aa2d3f: Layer already exists\n",
      "34db85e87fb0: Layer already exists\n",
      "3a4c764c9a45: Layer already exists\n",
      "782857218c62: Layer already exists\n",
      "195f2f019955: Layer already exists\n",
      "9546e4c87db8: Layer already exists\n",
      "97676aba1403: Layer already exists\n",
      "0fb4d6eba0b6: Layer already exists\n",
      "78613865172b: Layer already exists\n",
      "eb1f25078652: Layer already exists\n",
      "b7ce5de9980f: Layer already exists\n",
      "e8b9f9e4195c: Layer already exists\n",
      "0796fbb63752: Layer already exists\n",
      "1233ef617acb: Layer already exists\n",
      "390ace10d575: Layer already exists\n",
      "e05247f7d2f8: Layer already exists\n",
      "70720fcb790d: Layer already exists\n",
      "c1b078f0e002: Layer already exists\n",
      "5ccc1cf20429: Layer already exists\n",
      "ab7c37becce8: Layer already exists\n",
      "b2d48dbbbef2: Layer already exists\n",
      "260b0b2ff58a: Layer already exists\n",
      "6babb56be259: Layer already exists\n",
      "88a82020f9c7: Pushed\n",
      "89d38117114e: Pushed\n",
      "latest: digest: sha256:56d09438c36dcd660ea204686f0ea8cc73ad027b4fa035fb6b6961388ec4df72 size: 8519\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                      STATUS\n",
      "41d9c60f-43d9-4325-a4d8-19e1744a6de8  2022-11-18T09:44:15+00:00  9M7S      gs://qwiklabs-asl-01-579c20dd4e24_cloudbuild/source/1668764654.268241-df2fc4a287bc47cfa095a7bb68431d61.tgz  gcr.io/qwiklabs-asl-01-579c20dd4e24/tfx-guided-project-on-vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $CUSTOM_TFX_IMAGE ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Browse your copied source files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFX template provides basic scaffold files to build a pipeline, including Python source code,\n",
    "sample data, and Jupyter Notebooks to analyse the output of the pipeline. \n",
    "\n",
    "The `taxi` template uses the Chicago Taxi dataset.\n",
    "\n",
    "Here is brief introduction to each of the Python files:\n",
    "\n",
    "`pipeline` - This directory contains the definition of the pipeline\n",
    "* `configs.py` — defines common constants for pipeline runners\n",
    "* `pipeline.py` — defines TFX components and a pipeline\n",
    "\n",
    "`models` - This directory contains ML model definitions.\n",
    "* `features.py`, `features_test.py` — defines features for the model\n",
    "* `preprocessing.py`, `preprocessing_test.py` — defines preprocessing jobs using tf::Transform\n",
    "\n",
    "`models/estimator` - This directory contains an Estimator based model.\n",
    "* `constants.py` — defines constants of the model\n",
    "* `model.py`, `model_test.py` — defines DNN model using TF estimator\n",
    "\n",
    "`models/keras` - This directory contains a Keras based model.\n",
    "* `constants.py` — defines constants of the model\n",
    "* `model.py`, `model_test.py` — defines DNN model using Keras\n",
    "\n",
    "`local_runner.py`, `kubeflow_runner.py`, `kubeflow_v2_runner.py` — define runners for each orchestration engine\n",
    "\n",
    "\n",
    "**Running the tests:**\n",
    "You might notice that there are some files with `_test.py` in their name. \n",
    "These are unit tests of the pipeline and it is recommended to add more unit \n",
    "tests as you implement your own pipelines. \n",
    "You can run unit tests using the python `-m` and supplying the path to the test module. \n",
    "You can usually get a module name by deleting `.py` extension and replacing `/` with `..`\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-18 10:03:42.341977: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-18 10:03:42.342029: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-18 10:03:42.342051: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mlops): /proc/driver/nvidia/version does not exist\n",
      "Running tests under Python 3.7.12: /opt/conda/bin/python\n",
      "[ RUN      ] FeaturesTest.testNumberOfBucketFeatureBucketCount\n",
      "INFO:tensorflow:time(__main__.FeaturesTest.testNumberOfBucketFeatureBucketCount): 0.0s\n",
      "I1118 10:03:42.345040 140549603784512 test_util.py:2189] time(__main__.FeaturesTest.testNumberOfBucketFeatureBucketCount): 0.0s\n",
      "[       OK ] FeaturesTest.testNumberOfBucketFeatureBucketCount\n",
      "[ RUN      ] FeaturesTest.testTransformedNames\n",
      "INFO:tensorflow:time(__main__.FeaturesTest.testTransformedNames): 0.0s\n",
      "I1118 10:03:42.345425 140549603784512 test_util.py:2189] time(__main__.FeaturesTest.testTransformedNames): 0.0s\n",
      "[       OK ] FeaturesTest.testTransformedNames\n",
      "[ RUN      ] FeaturesTest.test_session\n",
      "[  SKIPPED ] FeaturesTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.001s\n",
      "\n",
      "OK (skipped=1)\n",
      "2022-11-18 10:03:45.749012: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-11-18 10:03:45.749061: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-18 10:03:45.749103: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mlops): /proc/driver/nvidia/version does not exist\n",
      "Running tests under Python 3.7.12: /opt/conda/bin/python\n",
      "[ RUN      ] ModelTest.testBuildKerasModel\n",
      "2022-11-18 10:03:45.779350: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I1118 10:03:45.863465 139699123222336 layer_utils.py:187] Model: \"model\"\n",
      "I1118 10:03:45.863682 139699123222336 layer_utils.py:188] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.863778 139699123222336 layer_utils.py:185] Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "I1118 10:03:45.863850 139699123222336 layer_utils.py:190] ==================================================================================================\n",
      "I1118 10:03:45.864119 139699123222336 layer_utils.py:185] pickup_latitude_xf (InputLayer) [(None,)]            0                                            \n",
      "I1118 10:03:45.864214 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.864389 139699123222336 layer_utils.py:185] trip_miles_xf (InputLayer)      [(None,)]            0                                            \n",
      "I1118 10:03:45.864465 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.864651 139699123222336 layer_utils.py:185] trip_start_hour_xf (InputLayer) [(None,)]            0                                            \n",
      "I1118 10:03:45.864722 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.864926 139699123222336 layer_utils.py:185] dense_features (DenseFeatures)  (None, 1)            0           pickup_latitude_xf[0][0]         \n",
      "I1118 10:03:45.865019 139699123222336 layer_utils.py:185]                                                                  trip_miles_xf[0][0]              \n",
      "I1118 10:03:45.865106 139699123222336 layer_utils.py:185]                                                                  trip_start_hour_xf[0][0]         \n",
      "I1118 10:03:45.865160 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.865449 139699123222336 layer_utils.py:185] dense (Dense)                   (None, 1)            2           dense_features[0][0]             \n",
      "I1118 10:03:45.865570 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.865796 139699123222336 layer_utils.py:185] dense_1 (Dense)                 (None, 1)            2           dense[0][0]                      \n",
      "I1118 10:03:45.865892 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.866098 139699123222336 layer_utils.py:185] dense_features_1 (DenseFeatures (None, 34)           0           pickup_latitude_xf[0][0]         \n",
      "I1118 10:03:45.866182 139699123222336 layer_utils.py:185]                                                                  trip_miles_xf[0][0]              \n",
      "I1118 10:03:45.866252 139699123222336 layer_utils.py:185]                                                                  trip_start_hour_xf[0][0]         \n",
      "I1118 10:03:45.866329 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.866491 139699123222336 layer_utils.py:185] concatenate (Concatenate)       (None, 35)           0           dense_1[0][0]                    \n",
      "I1118 10:03:45.866607 139699123222336 layer_utils.py:185]                                                                  dense_features_1[0][0]           \n",
      "I1118 10:03:45.866655 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.866872 139699123222336 layer_utils.py:185] dense_2 (Dense)                 (None, 1)            36          concatenate[0][0]                \n",
      "I1118 10:03:45.866968 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.867113 139699123222336 layer_utils.py:185] tf.compat.v1.squeeze (TFOpLambd (None,)              0           dense_2[0][0]                    \n",
      "I1118 10:03:45.867184 139699123222336 layer_utils.py:258] ==================================================================================================\n",
      "I1118 10:03:45.867602 139699123222336 layer_utils.py:269] Total params: 40\n",
      "I1118 10:03:45.867720 139699123222336 layer_utils.py:270] Trainable params: 40\n",
      "I1118 10:03:45.867802 139699123222336 layer_utils.py:271] Non-trainable params: 0\n",
      "I1118 10:03:45.867853 139699123222336 layer_utils.py:272] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.944800 139699123222336 layer_utils.py:187] Model: \"model_1\"\n",
      "I1118 10:03:45.944984 139699123222336 layer_utils.py:188] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.945078 139699123222336 layer_utils.py:185] Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "I1118 10:03:45.945152 139699123222336 layer_utils.py:190] ==================================================================================================\n",
      "I1118 10:03:45.945365 139699123222336 layer_utils.py:185] pickup_latitude_xf (InputLayer) [(None,)]            0                                            \n",
      "I1118 10:03:45.945460 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.945635 139699123222336 layer_utils.py:185] trip_miles_xf (InputLayer)      [(None,)]            0                                            \n",
      "I1118 10:03:45.945714 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.945864 139699123222336 layer_utils.py:185] trip_start_hour_xf (InputLayer) [(None,)]            0                                            \n",
      "I1118 10:03:45.945945 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.946148 139699123222336 layer_utils.py:185] dense_features_2 (DenseFeatures (None, 1)            0           pickup_latitude_xf[0][0]         \n",
      "I1118 10:03:45.946231 139699123222336 layer_utils.py:185]                                                                  trip_miles_xf[0][0]              \n",
      "I1118 10:03:45.946301 139699123222336 layer_utils.py:185]                                                                  trip_start_hour_xf[0][0]         \n",
      "I1118 10:03:45.946377 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.946654 139699123222336 layer_utils.py:185] dense_3 (Dense)                 (None, 1)            2           dense_features_2[0][0]           \n",
      "I1118 10:03:45.946744 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.947015 139699123222336 layer_utils.py:185] dense_features_3 (DenseFeatures (None, 34)           0           pickup_latitude_xf[0][0]         \n",
      "I1118 10:03:45.947136 139699123222336 layer_utils.py:185]                                                                  trip_miles_xf[0][0]              \n",
      "I1118 10:03:45.947217 139699123222336 layer_utils.py:185]                                                                  trip_start_hour_xf[0][0]         \n",
      "I1118 10:03:45.947264 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.947419 139699123222336 layer_utils.py:185] concatenate_1 (Concatenate)     (None, 35)           0           dense_3[0][0]                    \n",
      "I1118 10:03:45.947491 139699123222336 layer_utils.py:185]                                                                  dense_features_3[0][0]           \n",
      "I1118 10:03:45.947562 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.947776 139699123222336 layer_utils.py:185] dense_4 (Dense)                 (None, 1)            36          concatenate_1[0][0]              \n",
      "I1118 10:03:45.947893 139699123222336 layer_utils.py:260] __________________________________________________________________________________________________\n",
      "I1118 10:03:45.948031 139699123222336 layer_utils.py:185] tf.compat.v1.squeeze_1 (TFOpLam (None,)              0           dense_4[0][0]                    \n",
      "I1118 10:03:45.948117 139699123222336 layer_utils.py:258] ==================================================================================================\n",
      "I1118 10:03:45.948450 139699123222336 layer_utils.py:269] Total params: 38\n",
      "I1118 10:03:45.948571 139699123222336 layer_utils.py:270] Trainable params: 38\n",
      "I1118 10:03:45.948651 139699123222336 layer_utils.py:271] Non-trainable params: 0\n",
      "I1118 10:03:45.948701 139699123222336 layer_utils.py:272] __________________________________________________________________________________________________\n",
      "INFO:tensorflow:time(__main__.ModelTest.testBuildKerasModel): 0.2s\n",
      "I1118 10:03:45.949533 139699123222336 test_util.py:2189] time(__main__.ModelTest.testBuildKerasModel): 0.2s\n",
      "[       OK ] ModelTest.testBuildKerasModel\n",
      "[ RUN      ] ModelTest.test_session\n",
      "[  SKIPPED ] ModelTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.199s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "!python -m models.features_test\n",
    "!python -m models.keras_model.model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly go over the structure of a test file to test Tensorflow code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "from models import features\n",
      "\n",
      "\n",
      "class FeaturesTest(tf.test.TestCase):\n",
      "\n",
      "  def testNumberOfBucketFeatureBucketCount(self):\n",
      "    self.assertEqual(\n",
      "        len(features.BUCKET_FEATURE_KEYS),\n",
      "        len(features.BUCKET_FEATURE_BUCKET_COUNT))\n",
      "    self.assertEqual(\n",
      "        len(features.CATEGORICAL_FEATURE_KEYS),\n",
      "        len(features.CATEGORICAL_FEATURE_MAX_VALUES))\n",
      "\n",
      "  def testTransformedNames(self):\n",
      "    names = [\"f1\", \"cf\"]\n",
      "    self.assertEqual([\"f1_xf\", \"cf_xf\"], features.transformed_names(names))\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  tf.test.main()\n"
     ]
    }
   ],
   "source": [
    "!tail -26 models/features_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, notice that you start by importing the code you want to test by importing the corresponding module. Here we want to test the code in `features.py` so we import the module `features`:\n",
    "```python\n",
    "from models import features\n",
    "```\n",
    "To implement test cases start by defining your own test class inheriting from `tf.test.TestCase`:\n",
    "```python\n",
    "class FeaturesTest(tf.test.TestCase):\n",
    "```\n",
    "Wen you execute the test file with\n",
    "```bash\n",
    "python -m models.features_test\n",
    "```\n",
    "the main method\n",
    "```python\n",
    " tf.test.main()\n",
    "```\n",
    "will parse your test class (here: `FeaturesTest`) and execute every method whose name starts by `test`. Here we have two such methods for instance:\n",
    "```python\n",
    "def testNumberOfBucketFeatureBucketCount(self):\n",
    "def testTransformedNames(self):\n",
    "```\n",
    "So when you want to add a test case, just add a method to that test class whose name starts by `test`. Now inside the body of these test methods is where the actual testing takes place. In this case for instance, `testTransformedNames` test the function `features.transformed_name` and makes sure it outputs what is expected.\n",
    "Since your test class inherits from `tf.test.TestCase` it has a number of helper methods you can use to help you create tests, as for instance\n",
    "```python\n",
    "self.assertEqual(expected_outputs, obtained_outputs)\n",
    "```\n",
    "that will fail the test case if `obtained_outputs` do the match the `expected_outputs`. \n",
    "\n",
    "\n",
    "Typical examples of test case you may want to implement for machine learning code would comprise test insurring that your model builds correctly, your preprocessing function preprocesses raw data as expected, or that your model can train successfully on a few mock examples. When writing tests make sure that their execution is fast (we just want to check that the code works not actually train a performant model when testing). For that you may have to create synthetic data in your test files. For more information, read the [tf.test.TestCase documentation](https://www.tensorflow.org/api_docs/python/tf/test/TestCase) and the [Tensorflow testing best practices](https://www.tensorflow.org/community/contribute/tests).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Run your first TFX pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components in the TFX pipeline will generate outputs for each run as\n",
    "[ML Metadata Artifacts](https://www.tensorflow.org/tfx/guide/mlmd), and they need to be stored in a GCS bucket accessible from Vertex.\n",
    "\n",
    "\n",
    "Let us create this bucket. Its name will be `<YOUR_PROJECT>-kubeflowpipelines-default`.\n",
    "\n",
    "**Note:** The name of this bucket can be changed, but then it will also need to be changed in the generated `./pipeline/configs.py` file, which also defines a corresponding `GCS_BUCKET_NAME` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwiklabs-asl-01-579c20dd4e24-kubeflowpipelines-default'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + \"-kubeflowpipelines-default\"\n",
    "GCS_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create this bucket in case it does not exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-asl-01-579c20dd4e24-kubeflowpipelines-default/...\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^gs://{GCS_BUCKET_NAME}/$ || gsutil mb -l {REGION} gs://{GCS_BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload our sample data to GCS bucket so that we can use it in our pipeline later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/data.csv [Content-Type=text/csv]...\n",
      "/ [1 files][  1.8 MiB/  1.8 MiB]                                                \n",
      "Operation completed over 1 objects/1.8 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp data/data.csv gs://{GCS_BUCKET_NAME}/tfx-template/data/taxi/data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is now ready to be compiled and executed. You will produce `pipeline.json` artifact describing the template TFX pipeline and that can be executed on Vertex as a Vertex pipeline using the following compilation command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Compiling pipeline\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:absl:tensorflow_ranking is not available: No module named 'tensorflow_ranking'\n",
      "INFO:absl:tensorflow_text is not available: No module named 'tensorflow_text'\n",
      "INFO:absl:tensorflow_decision_forests is not available: No module named 'tensorflow_decision_forests'\n",
      "INFO:absl:struct2tensor is not available: No module named 'struct2tensor'\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:absl:tensorflow_text is not available: No module named 'tensorflow_text'\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "Pipeline tfx-guided-project-on-vertex compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline compile --engine vertex --pipeline_path kubeflow_v2_runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see a `pipeline.json` file in `PROJECT_DIR` (which should be the current working directory, since we `cd` into it earlier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline.json\n"
     ]
    }
   ],
   "source": [
    "ls pipeline.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch the execution of this pipeline on Vertex, we will use the `aiplatform` sdk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118100515\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118100515')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tfx-guided-project-on-vertex-20221118100515?project=296524281444\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118100515 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118100515 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118100515 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118100515 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118100515 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118100515 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118100515\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=\"pipeline.json\",\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline is minimal and only comprises the `CSVExampleGen` component. \n",
    "\n",
    "In the next sections, we will add more and more components to this pipeline by uncommenting and modifying the TFX scaffold generated by `tfx template`.\n",
    "\n",
    "You'll be able to see the pipeline runing at: https://console.cloud.google.com/vertex-ai/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Add components for data validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you will add components for data validation including `StatisticsGen`, `SchemaGen`, and `ExampleValidator`.\n",
    "If you are interested in data validation, please see \n",
    "[Get started with Tensorflow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double-click to change directory to pipeline and double-click again to open** `pipeline.py`. \n",
    "Find and uncomment the 3 lines which add `StatisticsGen`, `SchemaGen`, and `ExampleValidator` to the pipeline.\n",
    "(Tip: search for comments containing TODO(step 5):). Make sure to save `pipeline.py` after you edit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now need to update the existing pipeline with modified pipeline definition and trigger another run on Vertex (the cell above that runs the pipeline may need to be interrupted to allow for the execution of the two next cells) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Compiling pipeline\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:absl:tensorflow_ranking is not available: No module named 'tensorflow_ranking'\n",
      "INFO:absl:tensorflow_text is not available: No module named 'tensorflow_text'\n",
      "INFO:absl:tensorflow_decision_forests is not available: No module named 'tensorflow_decision_forests'\n",
      "INFO:absl:struct2tensor is not available: No module named 'struct2tensor'\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:absl:tensorflow_text is not available: No module named 'tensorflow_text'\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "Pipeline tfx-guided-project-on-vertex compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline compile --engine vertex --pipeline_path kubeflow_v2_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102431\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102431')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tfx-guided-project-on-vertex-20221118102431?project=296524281444\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102431 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102431 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102431 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102431\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=\"pipeline.json\",\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check pipeline outputs\n",
    "\n",
    "You'll be able to see the pipeline runing at: https://console.cloud.google.com/vertex-ai/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Add components for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you will add components for training and model validation including `Transform`, `Trainer`, `Resolver`, `Evaluator`, and `Pusher`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double-click to open** `pipeline.py`. Find and uncomment the 5 lines which add `Transform`, `Trainer`, `ResolverNode`, `Evaluator` and `Pusher` to the pipeline. (Tip: search for TODO(step 6):)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now need to update the existing pipeline with modified pipeline definition and trigger another run on Vertex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Compiling pipeline\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:absl:tensorflow_ranking is not available: No module named 'tensorflow_ranking'\n",
      "INFO:absl:tensorflow_text is not available: No module named 'tensorflow_text'\n",
      "INFO:absl:tensorflow_decision_forests is not available: No module named 'tensorflow_decision_forests'\n",
      "INFO:absl:struct2tensor is not available: No module named 'struct2tensor'\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:absl:tensorflow_text is not available: No module named 'tensorflow_text'\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "Pipeline tfx-guided-project-on-vertex compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline compile --engine vertex --pipeline_path kubeflow_v2_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102734\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102734')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tfx-guided-project-on-vertex-20221118102734?project=296524281444\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102734 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102734 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102734 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102734 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102734 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102734 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118102734\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=\"pipeline.json\",\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be able to see the pipeline runing at: https://console.cloud.google.com/vertex-ai/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Try BigQueryExampleGen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BigQuery](https://cloud.google.com/bigquery) is a serverless, highly scalable, and cost-effective cloud data warehouse.\n",
    "`BigQuery` can be used as a source for training examples in TFX. In this step, we will add `BigQueryExampleGen` to the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double-click to open** `pipeline.py`. Comment out `CsvExampleGen` and uncomment the line which creates an instance of `BigQueryExampleGen`. You also need to uncomment the query argument of the `create_pipeline` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify which GCP project to use for `BigQuery`, and this is done by setting `--project` in `beam_pipeline_args` when creating a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double-click to open** `configs.py`. Uncomment the definition of `GOOGLE_CLOUD_REGION`, `BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS` and `BIG_QUERY_QUERY`. You should replace the region value in this file with the correct values for your GCP project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You MUST set your GCP region in the `configs.py` file before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change directory one level up.** Click the name of the directory above the file list. The name of the directory is the name of the pipeline which is `tfx-guided-project-on-vertex` if you didn't change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double-click to open** `kubeflow_v2_runner.py`. Uncomment two arguments, `query` and `beam_pipeline_args`, for the `create_pipeline` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the pipeline is ready to use `BigQuery` as an example source. Update the pipeline as before and create a new execution run as we did in step 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Compiling pipeline\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:absl:tensorflow_ranking is not available: No module named 'tensorflow_ranking'\n",
      "INFO:absl:tensorflow_text is not available: No module named 'tensorflow_text'\n",
      "INFO:absl:tensorflow_decision_forests is not available: No module named 'tensorflow_decision_forests'\n",
      "INFO:absl:struct2tensor is not available: No module named 'struct2tensor'\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "INFO:absl:tensorflow_text is not available: No module named 'tensorflow_text'\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "Pipeline tfx-guided-project-on-vertex compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline compile --engine vertex --pipeline_path kubeflow_v2_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118104601\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118104601')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tfx-guided-project-on-vertex-20221118104601?project=296524281444\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118104601 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118104601 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118104601 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118104601 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118104601 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118104601 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118104601 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/296524281444/locations/us-central1/pipelineJobs/tfx-guided-project-on-vertex-20221118104601\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=REGION)\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=\"pipeline.json\",\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll be able to see the pipeline runing at: https://console.cloud.google.com/vertex-ai/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Customize the pipeline to your data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a TFX pipeline for a model using the Chicago Taxi dataset. Now it's time to put your data into the pipeline.\n",
    "\n",
    "\n",
    "\n",
    "Your data can be stored anywhere your pipeline can access, including GCS, or BigQuery. You will need to modify the pipeline definition to access your data.\n",
    "\n",
    "Review the steps above to remember what needs to be customized in full details. You'll find below a short summary of these steps:\n",
    "\n",
    "1. If your data is stored in files, modify the `DATA_PATH` in `kubeflow_v2_runner.py` and set it to the location of your files. If your data is stored in BigQuery, modify `BIG_QUERY_QUERY` in `pipeline/configs.py` to correctly query for your data.\n",
    "1. Add features in `models/features.py`\n",
    "1. Modify `models/preprocessing.py` to [transform input data for training](https://www.tensorflow.org/tfx/guide/transform).\n",
    "1. Modify `models/keras/model.py` and `models/keras/constants.py` to [describe your ML model](https://www.tensorflow.org/tfx/guide/trainer).\n",
    "\n",
    "We suggest that you take a small sample of the data, and select columns that are easy to preprocess for the sake of time. Here are a few pointers to get inspiration:\n",
    "\n",
    "* [A small Slice](https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/?date_received_max=2020-11-26&date_received_min=2020-08-26&field=all&format=csv&no_aggs=true&size=119459) of the [Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/). (You'll still probably need to take only a subset of the rows and columns for the sake of fast model developpement.)\n",
    "* The [Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) that has a number of very interesting datasets.\n",
    "\n",
    "The easiest way to create a small CVS file containing your dataset in the Jupyterlab, and then upload it to in a Cloud Storage bucket. This way you'll simply use `CsvExampleGen` to connect to your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## License\n",
    "\n",
    "Copyright 2022 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
